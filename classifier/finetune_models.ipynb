{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##NOTE:\n",
        "\n",
        "To run this notebook, the file 'final_dataset.csv' is needed."
      ],
      "metadata": {
        "id": "YoNwZppTg-R7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7_UzjgSdTIr"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "m0Adi2p7tuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WaGCVePHfBT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read in data set"
      ],
      "metadata": {
        "id": "VUe02Qauu0kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Value, ClassLabel, Features, DatasetDict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('final_dataset.csv')\n",
        "df.columns = ['sentence', 'original_label', 'thread_id', 'comment_id', 'label']\n",
        "\n",
        "df = df.drop_duplicates(subset=['thread_id', 'sentence'])"
      ],
      "metadata": {
        "id": "gPuDUVoOu3ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data into training, validation and test set"
      ],
      "metadata": {
        "id": "pvQ7qcwcvC2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select test and validation thread ids\n",
        "test_ids = ['t3_6rwcio', 't3_5jfqhp', 't3_71l9yj', 't3_4mj8v7', 't3_58t7i3',\n",
        "            't3_64kkxe', 't3_6ihcuk', 't3_5o7nm3', 't3_4tf91m', 't3_4q9qng']\n",
        "val_ids = ['t3_5ep0mh', 't3_4pbwvb', 't3_4g3nbn', 't3_6tsx1p', 't3_62igvv',\n",
        "           't3_6694ui', 't3_6h7a4i', 't3_4plwqq', 't3_4otmqi', 't3_57tl4k']\n",
        "\n",
        "# Encode labels into integers\n",
        "labels = df.label.unique()\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "label2id = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "id2label = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
        "\n",
        "label2id = {label:int(id) for label,id in label2id.items()}\n",
        "id2label = {int(id):label for id,label in id2label.items()}\n",
        "\n",
        "train_df = df[~(df['thread_id'].isin((test_ids + val_ids)))][['sentence', 'label']]\n",
        "val_df = df[df['thread_id'].isin(val_ids)][['sentence', 'label']]\n",
        "test_df = df[df['thread_id'].isin(test_ids)][['sentence', 'label']]\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "evidence_dataset = DatasetDict({'train': train_ds, 'validation': val_ds, 'test': test_ds})"
      ],
      "metadata": {
        "id": "FXN2Y97rDId_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "Wtsdbnc6haQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'microsoft/MiniLM-L12-H384-uncased'\n",
        "# model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "t2aP8pTIhfgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "    return tokenizer(sentences['sentence'], truncation=True, max_length=512)"
      ],
      "metadata": {
        "id": "-_XZKYleiRn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_dataset = evidence_dataset.map(tokenize_sentences, batched=True)"
      ],
      "metadata": {
        "id": "WUuy7sUkjyI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding weighted loss function"
      ],
      "metadata": {
        "id": "RNcP3kuij4ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class_weights = (1 - (df['label'].value_counts().sort_index() / len(df))).values\n",
        "class_weights = torch.from_numpy(class_weights).float().to('cuda')"
      ],
      "metadata": {
        "id": "vHVOJpL3jwQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717dfb78-3d76-43d2-affd-e18a29c261ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8901, 0.3986, 0.9784, 0.7569, 0.9849, 0.9911], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_dataset = evidence_dataset.rename_column('label', 'labels')"
      ],
      "metadata": {
        "id": "FuBBNYNUlJeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "\n",
        "class TrainerImbalancedData(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Overwrite compute_loss function to use class weights\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "        labels = inputs.get('labels')\n",
        "        loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_function(logits, labels)\n",
        "        if return_outputs:\n",
        "            return (loss, outputs)\n",
        "        else:\n",
        "            return loss"
      ],
      "metadata": {
        "id": "N2A7V_YMlVWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels), id2label=id2label, label2id=label2id)"
      ],
      "metadata": {
        "id": "gj5zvIgPmhGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
        "predictions_df = pd.DataFrame()\n",
        "predictions_df['true'] = val_df.label\n",
        "def compute_metrics(predictions):\n",
        "    labels = predictions.label_ids    \n",
        "    predictions = predictions.predictions.argmax(-1)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    b_acc = balanced_accuracy_score(labels, predictions)\n",
        "    predictions_df[len(predictions_df.columns)] = predictions\n",
        "    return { 'macro f1': f1_macro, 'weighted f1': f1_weighted, 'accuracy': acc, 'balanced accuracy': b_acc}"
      ],
      "metadata": {
        "id": "NWWqaBJSnMwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "output_dir = 'MiniLM-evidence-types'\n",
        "# output_dir = 'BERT-evidence-types'\n",
        "training_args = TrainingArguments(output_dir = output_dir,\n",
        "                                  num_train_epochs=epochs,\n",
        "                                  learning_rate=3e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy='epoch',\n",
        "                                  logging_strategy='epoch',\n",
        "                                  save_strategy='epoch',\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='macro f1',\n",
        "                                  fp16=True,\n",
        "                                  seed=42)"
      ],
      "metadata": {
        "id": "yKJPjHKan0Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerImbalancedData(model_init=model_init,\n",
        "                  args=training_args,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset=evidence_dataset['train'],\n",
        "                  eval_dataset=evidence_dataset['validation'],\n",
        "                  tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "9syqkThSon5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "hDIIHS1fpDT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions_df.replace(id2label).to_csv('predictions_MiniLM_bs32_lr2e-5.csv', index=False)"
      ],
      "metadata": {
        "id": "uHx_ieKXD25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "LwpEg2AqzstH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}